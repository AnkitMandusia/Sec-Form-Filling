{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12856234,"sourceType":"datasetVersion","datasetId":8131656}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:37:27.796377Z","iopub.execute_input":"2025-09-01T08:37:27.796624Z","iopub.status.idle":"2025-09-01T08:37:27.801313Z","shell.execute_reply.started":"2025-09-01T08:37:27.796604Z","shell.execute_reply":"2025-09-01T08:37:27.800697Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# --- 1. Install Dependencies ---\n# Using quiet mode to keep the output clean\n!pip install -q \\\n    transformers \\\n    accelerate \\\n    bitsandbytes \\\n    sentence-transformers \\\n    pypdf \\\n    unstructured[html] \\\n    networkx \\\n    matplotlib \\\n    streamlit \\\n    langchain \\\n    langchain-community \\\n    langchain-huggingface \\\n    qdrant-client \\\n    altair \\\n    pandas \\\n    bitsandbytes \\\n    pyngrok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:37:27.807520Z","iopub.execute_input":"2025-09-01T08:37:27.808183Z","iopub.status.idle":"2025-09-01T08:39:10.010432Z","shell.execute_reply.started":"2025-09-01T08:37:27.808122Z","shell.execute_reply":"2025-09-01T08:39:10.009690Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: unstructured 0.18.14 does not provide the extra 'html'\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 24.1.0 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install plotly","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:39:10.012049Z","iopub.execute_input":"2025-09-01T08:39:10.012316Z","iopub.status.idle":"2025-09-01T08:39:13.071565Z","shell.execute_reply.started":"2025-09-01T08:39:10.012292Z","shell.execute_reply":"2025-09-01T08:39:13.070833Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (25.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- 2. Load Models, Process Data, and Build Resources ---\nimport os\nimport torch\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.llms import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain_community.document_loaders import UnstructuredHTMLLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Qdrant\nimport networkx as nx\nimport re\nimport requests\nimport json\nimport altair as alt\n\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --- Hugging Face Token Setup ---\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n    os.environ['HUGGING_FACE_HUB_TOKEN'] = hf_token\nexcept Exception as e:\n    print(\"Could not retrieve Hugging Face token. Make sure it's set in Kaggle Secrets.\", e)\n\n# --- Document Download and Processing ---\n# This uses the correct, modern HTML version of the SEC filing\nfile_url = \"https://www.sec.gov/Archives/edgar/data/789019/000095017024087843/msft-20240630.htm\"\nfile_path = \"msft-20240630.htm\"\nheaders = {'User-Agent': \"MyKaggleProject myemail@example.com\"}\n\nprint(f\"Downloading file from {file_url}...\")\nresponse = requests.get(file_url, headers=headers)\nif response.status_code == 200:\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(response.text)\n    print(\"Download complete.\")\n    loader = UnstructuredHTMLLoader(file_path)\n    documents = loader.load()\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n    docs = text_splitter.split_documents(documents)\n    print(f\"Document loaded and split into {len(docs)} chunks.\")\nelse:\n    print(f\"Failed to download file. Status code: {response.status_code}\")\n    docs = []\n\n# --- Model Loading ---\nllm_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\nprint(\"Loading LLM...\")\nbnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\nmodel = AutoModelForCausalLM.from_pretrained(llm_model_name, quantization_config=bnb_config, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n\n# --- Create the Transformers Pipeline ---\n# This defines the `text_generation_pipeline`\nprint(\"Creating transformers pipeline...\")\ntext_generation_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=1024,\n    top_p=0.95,\n    temperature=0.1,\n    repetition_penalty=1.15,\n    return_full_text=False # Important for clean Gradio/Streamlit output\n)\n\n# --- THIS IS THE CRITICAL STEP THAT FIXES THE ERROR ---\n.\nllm_pipeline = HuggingFacePipeline(pipeline=text_generation_pipeline)\nprint(\"`llm_pipeline` created successfully.\")\n\n# --- Now you can safely use llm_pipeline ---\n\ngraph_prompt_template = \"...\" # Your prompt template here\ngraph_prompt = PromptTemplate.from_template(graph_prompt_template)\n\nprint(\"\\nAll components are ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:39:13.072603Z","iopub.execute_input":"2025-09-01T08:39:13.072895Z","iopub.status.idle":"2025-09-01T08:42:45.002169Z","shell.execute_reply.started":"2025-09-01T08:39:13.072861Z","shell.execute_reply":"2025-09-01T08:42:45.001565Z"}},"outputs":[{"name":"stderr","text":"2025-09-01 08:39:30.581166: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756715970.964254      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756715971.079842      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Downloading file from https://www.sec.gov/Archives/edgar/data/789019/000095017024087843/msft-20240630.htm...\nDownload complete.\nDocument loaded and split into 375 chunks.\nLoading LLM...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4720749315d47738da5fb5b75c57127"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98404673c3004087b85cb99b267592d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b83d9dc7e264c559a72d5ec516fe50a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d65252489174418bcf9fcd490574518"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"926c66c539f7424a971f4a15e635fa54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83d0bedb4ef849f18edaa31656e6707f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9d21e3621c744d8ba38fdd6730b6537"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"002ec77ec11b482fa9c339e4195f13b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee8446fb90d6478980b71d55d68f5a07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25df11e5fb1c47c496ffebf4a50d788c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d2ec96eca304dc6a3cd54519848c16b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f46966153c5d482295206f9d56b2b027"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Creating transformers pipeline...\n`llm_pipeline` created successfully.\n\nAll components are ready.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile app.py\nimport streamlit as st\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextIteratorStreamer\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import UnstructuredHTMLLoader\nfrom langchain_community.vectorstores import Qdrant\nfrom threading import Thread\nimport pandas as pd\nimport io\nimport re\nimport plotly.express as px\nimport json\n\n# --- Page Configuration ---\nst.set_page_config(page_title=\"Kepler Financial Analyst\", page_icon=\"ğŸª\", layout=\"wide\")\n\n# --- Definitive UI Styling ---\nst.markdown(\"\"\"\n<style>\n    /* Set all text color to be visible on white background */\n    body, .st-emotion-cache-1kyxreq, .st-emotion-cache-1y4p8pa, .st-emotion-cache-1629p8f, .st-emotion-cache-1wivap2, .st-emotion-cache-4oy321 p, .st-emotion-cache-1c7y2kd p {\n        color: #111111 !important;\n    }\n    /* Titles and Headers */\n    h1, h2, h3 {\n        color: #000000;\n    }\n    /* Sidebar */\n    .st-emotion-cache-163cm81 {\n        background-color: #F0F2F6;\n    }\n    /* Chat Bubbles */\n    .st-emotion-cache-1c7y2kd { /* Assistant bubble */\n        background-color: #F0F2F6;\n    }\n    .st-emotion-cache-4oy321 { /* User bubble */\n        background-color: #FFFBEA; /* Light yellow */\n    }\n    /* Buttons */\n    .stButton > button {\n        background-color: #FFD700; /* Yellow */\n        color: #111111; /* Black text for buttons */\n        border-radius: 12px;\n        font-weight: bold;\n    }\n    .stButton > button:hover {\n        background-color: #FFC700;\n    }\n    /* Stop button specific style */\n    .st-emotion-cache-19n6bn1 { /* This targets the stop button specifically */\n        background-color: #D32F2F !important; /* Red */\n        color: white !important;\n    }\n</style>\n\"\"\", unsafe_allow_html=True)\n\n# --- State Management ---\nif \"messages\" not in st.session_state:\n    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"Hello! I am an AI analyst. How can I help you analyze or visualize the loaded financial document?\"}]\nif \"is_generating\" not in st.session_state:\n    st.session_state.is_generating = False\nif \"stop_generation\" not in st.session_state:\n    st.session_state.stop_generation = False\n\n# --- Model and Resources Caching ---\n@st.cache_resource\ndef load_resources():\n    llm_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n    model = AutoModelForCausalLM.from_pretrained(llm_model_name, quantization_config=bnb_config, device_map=\"auto\")\n    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n    file_path = \"msft-20240630.htm\"\n    \n    # Load tables from HTML\n    try:\n        tables = pd.read_html(file_path)\n        tables = [t.dropna(how='all').dropna(axis=1, how='all').fillna(0) for t in tables if not t.empty]\n    except Exception as e:\n        st.warning(f\"Failed to read tables from HTML file: {e}\")\n        tables = []\n    \n    loader = UnstructuredHTMLLoader(file_path)\n    documents = loader.load()\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n    docs = text_splitter.split_documents(documents)\n    vectorstore = Qdrant.from_documents(docs, embedding_model, location=\":memory:\", collection_name=\"sec_filing\")\n    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n    return model, tokenizer, retriever, tables\n\nmodel, tokenizer, retriever, financial_tables = load_resources()\n\n# --- AI Agent & Helper Functions ---\ndef get_llm_response(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=1024, pad_token_id=tokenizer.eos_token_id)\n    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if prompt in response_text:\n        return response_text.split(prompt)[-1].strip()\n    return response_text.strip()\n\ndef stream_llm_response(prompt):\n    st.session_state.stop_generation = False\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=1024, pad_token_id=tokenizer.eos_token_id)\n    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n    thread.start()\n    for new_text in streamer:\n        if st.session_state.stop_generation:\n            break\n        yield new_text\n\ndef classify_intent(user_prompt: str) -> str:\n    \"\"\"A deterministic, rule-based classifier for user intent with LLM fallback for ambiguous cases.\"\"\"\n    user_prompt = user_prompt.lower()\n    chart_keywords = [\"chart\", \"plot\", \"graph\", \"visualize\", \"draw\", \"show me a chart\"]\n    data_terms = [\"trend\", \"compare\", \"over time\", \"bar\", \"line\", \"pie\"]\n    \n    # Explicit chart intent\n    if any(kw in user_prompt for kw in chart_keywords):\n        return \"chart\"\n    # Implicit chart intent (e.g., \"show trend\" or \"compare X and Y\")\n    if any(term in user_prompt for term in data_terms) and any(word in user_prompt for word in [\"show\", \"display\", \"visual\"]):\n        return \"chart\"\n    # Fallback to LLM for ambiguous cases\n    router_prompt = f'You are a router. Based on the user question, is the intent \"text\" or \"chart\"? Respond with a single word.\\nQuestion: \"{user_prompt}\"\\nResponse:'\n    intent = get_llm_response(router_prompt).lower().strip()\n    return intent if intent in [\"text\", \"chart\"] else \"text\"\n\n# --- UI Layout ---\nst.title(\"ğŸª Kepler Financial Analyst\")\n\n# Display chat history\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        if \"chart\" in message:\n            st.plotly_chart(message[\"chart\"], use_container_width=True)\n        else:\n            st.markdown(message[\"content\"])\n\n# --- Main Logic Execution ---\nif st.session_state.is_generating:\n    last_user_prompt = st.session_state.messages[-1][\"content\"]\n    \n    with st.chat_message(\"assistant\"):\n        with st.status(\"Analyzing request...\", expanded=True) as status:\n            status.update(label=\"Retrieving context from document...\")\n            retrieved_docs = retriever.invoke(last_user_prompt)\n            retrieved_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n            \n            status.update(label=\"Classifying user intent...\")\n            intent = classify_intent(last_user_prompt)\n            \n            if intent == \"chart\":\n                status.update(label=\"Chart requested. Classifying chart type...\")\n                chart_type_prompt = f'You are a chart type classifier. Is the chart \"time-series\" or \"comparison\"? Respond with a single word.\\nQuestion: \"{last_user_prompt}\"\\nResponse:'\n                chart_type = get_llm_response(chart_type_prompt).lower().strip()\n                \n                status.update(label=\"Extracting data for chart...\")\n                if \"comparison\" in chart_type:\n                    extractor_prompt = f\"\"\"\n                    Extract data for a comparison chart. Respond ONLY in a <chart_data> tag with CSV data ('Category,Value').\n                    Do NOT include any other tags, JSON, or text outside the <chart_data> tag.\n                    Source Text: {retrieved_text}\n                    Question: \"{last_user_prompt}\"\n                    Response:\n                    \"\"\"\n                else:\n                    extractor_prompt = f\"\"\"\n                    Extract data for a time-series chart. Respond ONLY in a <chart_data> tag with CSV data ('Year,Value').\n                    Do NOT include any other tags, JSON, or text outside the <chart_data> tag.\n                    Source Text: {retrieved_text}\n                    Question: \"{last_user_prompt}\"\n                    Response:\n                    \"\"\"\n                \n                response_str = get_llm_response(extractor_prompt)\n                \n                status.update(label=\"Building visualization with Plotly...\")\n                try:\n                    # Robust regex to handle whitespace and malformed tags\n                    match = re.search(r'<chart_data>\\s*(.*?)\\s*</chart_data>', response_str, re.DOTALL)\n                    if not match:\n                        raise ValueError(\"No chart data found in the AI response.\")\n                    \n                    csv_data = match.group(1).strip()\n                    if not csv_data:\n                        raise ValueError(\"Chart data is empty.\")\n                    \n                    # Check for unexpected tags\n                    if '<bar_chart>' in response_str:\n                        st.warning(\"LLM included unexpected <bar_chart> tag. Ignoring and using <chart_data>.\")\n                        response_str = re.sub(r'<bar_chart>.*?</bar_chart>', '', response_str, flags=re.DOTALL)\n                    \n                    # Try table-based extraction as fallback\n                    target_df = None\n                    x_axis_label = 'year' if 'time-series' in chart_type else 'category'\n                    y_axis_label = 'value'\n                    for table in financial_tables:\n                        table.columns = [str(col).lower().strip() for col in table.columns]\n                        if x_axis_label in table.columns and y_axis_label in table.columns:\n                            target_df = table[[x_axis_label, y_axis_label]].copy()\n                            target_df[y_axis_label] = pd.to_numeric(target_df[y_axis_label], errors='coerce')\n                            target_df.dropna(inplace=True)\n                            break\n                    \n                    if target_df is None or target_df.empty:\n                        # Fallback to LLM-extracted CSV\n                        df = pd.read_csv(io.StringIO(csv_data))\n                    else:\n                        df = target_df\n                    \n                    x_col, y_col = df.columns[0], df.columns[1]\n                    fig = px.bar(df, x=x_col, y=y_col, title=last_user_prompt, text_auto=True)\n                    fig.update_xaxes(type='category')\n                    fig.update_layout(title_x=0.5, xaxis_title=x_col.replace(\"_\", \" \").title(), yaxis_title=y_col.replace(\"_\", \" \").title())\n                    \n                    st.plotly_chart(fig, use_container_width=True)\n                    st.session_state.messages.append({\"role\": \"assistant\", \"chart\": fig})\n                except Exception as e:\n                    error_message = f\"I tried to create a chart, but failed. Error: {e}\\n\\nHere is the raw data I received:\\n```\\n{response_str}\\n```\"\n                    st.error(error_message)\n                    # Fallback to text response\n                    status.update(label=\"Falling back to text response...\")\n                    answer_prompt = f\"You are a financial analyst AI. Answer the question based on the 'Source Text', followed by a 'Source Citation'.\\nSource Text:\\n{retrieved_text}\\nQuestion: \\\"{last_user_prompt}\\\"\\nAnswer:\"\n                    response_generator = stream_llm_response(answer_prompt)\n                    full_response = st.write_stream(response_generator)\n                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_message + \"\\n\\n\" + full_response})\n            else:\n                status.update(label=\"Generating text-based answer...\")\n                answer_prompt = f\"You are a financial analyst AI. Answer the question based on the 'Source Text', followed by a 'Source Citation'.\\nSource Text:\\n{retrieved_text}\\nQuestion: \\\"{last_user_prompt}\\\"\\nAnswer:\"\n                response_generator = stream_llm_response(answer_prompt)\n                full_response = st.write_stream(response_generator)\n                st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n            \n            status.update(label=\"Done!\", state=\"complete\", expanded=False)\n    st.session_state.is_generating = False\n    st.rerun()\n\n# --- Dynamic Input Area ---\nst.markdown(\"---\")\nif st.session_state.is_generating:\n    if st.button(\"â–  Stop Generation\", use_container_width=True, type=\"primary\"):\n        st.session_state.is_generating = False\n        st.session_state.stop_generation = True\n        st.rerun()\nelse:\n    if prompt := st.chat_input(\"Ask a question, e.g., 'Chart the net income...'\"):\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n        st.session_state.is_generating = True\n        st.rerun()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:42:45.003645Z","iopub.execute_input":"2025-09-01T08:42:45.004377Z","iopub.status.idle":"2025-09-01T08:42:45.014112Z","shell.execute_reply.started":"2025-09-01T08:42:45.004357Z","shell.execute_reply":"2025-09-01T08:42:45.013437Z"}},"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- 4. Extract Information and Build Knowledge Graph (Corrected) ---\nimport re # Import the regular expressions library\n\nprint(\"Extracting entities and relationships to build the knowledge graph...\")\n\n# We are using the same prompt template as before\ngraph_prompt_template = \"\"\"\nYou are a network graph maker. Your task is to extract entities and their relationships from a given text.\nYou must extract the full entity name. If a relationship is not explicitly mentioned, do not create one.\nFormat your output as a list of tuples, where each tuple represents a relationship: ('entity1', 'relationship', 'entity2').\nDo not add any explanation or text before or after the list.\n\nExample:\nText: Microsoft, a technology company, announced a partnership with OpenAI to develop new AI products.\nOutput: [('Microsoft', 'is a', 'technology company'), ('Microsoft', 'partnered with', 'OpenAI')]\n\nText: {chunk}\nOutput:\n\"\"\"\n\ngraph_prompt = PromptTemplate.from_template(graph_prompt_template)\n\n# Create the LLM chain for graph extraction\ngraph_extraction_chain = LLMChain(llm=llm_pipeline, prompt=graph_prompt)\n\n# Initialize the knowledge graph\nG = nx.DiGraph()\n\n# Process a subset of chunks to build the graph\nchunks_to_process = docs[:30]\n\nfor i, chunk in enumerate(chunks_to_process):\n    print(f\"Processing chunk {i+1}/{len(chunks_to_process)}...\")\n    \n    response = graph_extraction_chain.run(chunk.page_content)\n    \n    # --- NEW ROBUST PARSING LOGIC ---\n    # We use regex to find all tuples in the format ('entity1', 'relation', 'entity2')\n    # This is much more resilient to errors than eval()\n    try:\n        # The pattern looks for a parenthesis, a single-quoted string, a comma, \n        # another single-quoted string, a comma, a final single-quoted string, and a closing parenthesis.\n        pattern = r\"\\('([^']*)',\\s*'([^']*)',\\s*'([^']*)'\\)\"\n        triplets = re.findall(pattern, response)\n        \n        if triplets:\n            for subject, predicate, obj in triplets:\n                # Clean up the extracted strings\n                subject = subject.strip()\n                predicate = predicate.strip()\n                obj = obj.strip()\n                if subject and predicate and obj: # Ensure no empty strings\n                    G.add_edge(subject, obj, label=predicate)\n    except Exception as e:\n        print(f\"Error processing chunk {i+1}: {e}\")\n        continue\n\nprint(f\"Knowledge graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:42:45.014972Z","iopub.execute_input":"2025-09-01T08:42:45.015234Z","iopub.status.idle":"2025-09-01T08:48:37.198657Z","shell.execute_reply.started":"2025-09-01T08:42:45.015215Z","shell.execute_reply":"2025-09-01T08:48:37.198017Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Extracting entities and relationships to build the knowledge graph...\nProcessing chunk 1/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 2/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 3/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 4/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 5/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 6/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 7/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 8/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 9/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 10/30...\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 11/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 12/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 13/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 14/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 15/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 16/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 17/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 18/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 19/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 20/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 21/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 22/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 23/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 24/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 25/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 26/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 27/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 28/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 29/30...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing chunk 30/30...\nKnowledge graph created with 208 nodes and 161 edges.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pkill -f ngrok\n# This command finds the process using port 8501 and terminates it.\n!kill $(lsof -t -i:8501)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T09:03:29.676698Z","iopub.execute_input":"2025-09-01T09:03:29.677003Z","iopub.status.idle":"2025-09-01T09:03:30.976474Z","shell.execute_reply.started":"2025-09-01T09:03:29.676975Z","shell.execute_reply":"2025-09-01T09:03:30.975504Z"}},"outputs":[{"name":"stdout","text":"kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# --- Final Step: Launch the App and Get the Link ---\nimport os\nimport subprocess\nfrom pyngrok import ngrok\nfrom kaggle_secrets import UserSecretsClient\n\n\n\n# --- Set up ngrok ---\n# This ensures the token is ready. Since you've already configured it, this will just confirm.\n\ntry:\n    user_secrets = UserSecretsClient()\n    ngrok_token = user_secrets.get_secret(\"NGROK_AUTHTOKEN\")\n    ngrok.set_auth_token(ngrok_token)\n    print(\"Ngrok authtoken configured successfully.\")\nexcept Exception as e:\n    print(f\"Could not configure ngrok authtoken: {e}\")\n\n# --- Launch the Streamlit App in the background ---\nprint(\"Launching Streamlit app in the background...\")\nprocess = subprocess.Popen(['streamlit', 'run', 'app.py', '--server.port', '8501'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n# --- Get the Public URL from ngrok and Print It ---\ntry:\n    public_url = ngrok.connect(8501)\n    print(\"---\" * 20)\n    print(f\"âœ… Your Streamlit app is LIVE at: {public_url}\")\n    print(\"---\" * 20)\nexcept Exception as e:\n    print(f\"Could not connect ngrok. Error: {e}\")\n    # If it fails, kill the streamlit process\n    process.kill()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:48:38.686089Z","iopub.execute_input":"2025-09-01T08:48:38.686369Z","iopub.status.idle":"2025-09-01T08:48:40.034980Z","shell.execute_reply.started":"2025-09-01T08:48:38.686343Z","shell.execute_reply":"2025-09-01T08:48:40.034195Z"}},"outputs":[{"name":"stdout","text":"Ngrok authtoken configured successfully.                                                            \nLaunching Streamlit app in the background...\n------------------------------------------------------------\nâœ… Your Streamlit app is LIVE at: NgrokTunnel: \"https://b0ae027d2b1d.ngrok-free.app\" -> \"http://localhost:8501\"\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"Visualize the change in net income over the last three years as a bar chart.\n\n\"Plot the common stock repurchases for 2023 and 2024.\"\n\n\"Can you create a chart comparing cash dividends and stock repurchases?\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:48:40.035853Z","iopub.execute_input":"2025-09-01T08:48:40.036266Z","iopub.status.idle":"2025-09-01T08:48:40.042445Z","shell.execute_reply.started":"2025-09-01T08:48:40.036240Z","shell.execute_reply":"2025-09-01T08:48:40.041186Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_36/1546121222.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Visualize the change in net income over the last three years as a bar chart.\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (1546121222.py, line 1)","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"What was the net income for the most recent fiscal year?\nHow did the amount spent on common stock repurchases change between 2023 and 2024?\nWhat was the declared cash dividend per share in 2022?\nIs there a mention of seasonality affecting the company's revenue? ## Business Segments & Strategy \n\n#These questions explore the company's operations and future plans. \nWhat are the company's main reportable segments?\nWhat does the report say about the company's investments in Artificial Intelligence?\nWere there any significant acquisitions or partnerships mentioned in the document? \nHow did the company change its estimate for the useful lives of server equipment?\n\n## Risk Factors âš ï¸ These questions probe the \"Risk Factors\" section, which is crucial for understanding potential challenges\nWhat are the top 3 business risks identified by the company? \nDoes the report mention any risks related to cybersecurity or data breaches? \nWhat are the potential impacts of global competition on the company's business? \nAre there any legal proceedings mentioned that could materially harm the company?","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T08:48:40.043068Z","iopub.status.idle":"2025-09-01T08:48:40.043450Z","shell.execute_reply.started":"2025-09-01T08:48:40.043280Z","shell.execute_reply":"2025-09-01T08:48:40.043298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}